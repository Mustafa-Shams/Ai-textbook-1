"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[8581],{5610(e){e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Introduction","href":"/Ai-textbook-1/docs/","docId":"index","unlisted":false},{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Core Concepts: Nodes, Topics, Services","href":"/Ai-textbook-1/docs/module-1/concepts","docId":"module-1/concepts","unlisted":false},{"type":"link","label":"rclpy Implementation Examples","href":"/Ai-textbook-1/docs/module-1/rclpy-examples","docId":"module-1/rclpy-examples","unlisted":false},{"type":"link","label":"URDF Structures for Humanoid Joints/Links","href":"/Ai-textbook-1/docs/module-1/urdf-structures","docId":"module-1/urdf-structures","unlisted":false}],"href":"/Ai-textbook-1/docs/module-1/"},{"type":"category","label":"Module 2: The Digital Twin","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Tools: Gazebo & Unity","href":"/Ai-textbook-1/docs/module-2/tools","docId":"module-2/tools","unlisted":false},{"type":"link","label":"Sensors: LiDAR and Depth Cameras","href":"/Ai-textbook-1/docs/module-2/sensors","docId":"module-2/sensors","unlisted":false}],"href":"/Ai-textbook-1/docs/module-2/"},{"type":"category","label":"Module 3: The AI-Robot Brain","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"NVIDIA Isaac Sim & Isaac ROS","href":"/Ai-textbook-1/docs/module-3/isaac-sim","docId":"module-3/isaac-sim","unlisted":false},{"type":"link","label":"Nav2 Stack for Bipedal Movement","href":"/Ai-textbook-1/docs/module-3/nav2-movement","docId":"module-3/nav2-movement","unlisted":false}],"href":"/Ai-textbook-1/docs/module-3/"},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Integration: Whisper -> LLM -> ROS 2 Actions","href":"/Ai-textbook-1/docs/module-4/integration","docId":"module-4/integration","unlisted":false}],"href":"/Ai-textbook-1/docs/module-4/"},{"type":"link","label":"Capstone Project: The Autonomous Humanoid","href":"/Ai-textbook-1/docs/capstone/","docId":"capstone/index","unlisted":false}]},"docs":{"capstone/index":{"id":"capstone/index","title":"Capstone Project: The Autonomous Humanoid","description":"The capstone project represents the culmination of your learning journey in Physical AI and Humanoid Robotics. This comprehensive project integrates all concepts learned throughout the course to create a fully autonomous humanoid system that demonstrates embodied intelligence principles. The project challenges you to synthesize knowledge from ROS 2 communication, digital twin simulation, AI integration, and multimodal interaction into a cohesive, functioning robotic system.","sidebar":"tutorialSidebar"},"index":{"id":"index","title":"Introduction","description":"Welcome to the Physical AI & Humanoid Robotics textbook. This comprehensive guide explores the fascinating field of embodied intelligence, where artificial intelligence meets physical robotics. This convergence represents a paradigm shift from traditional AI systems that operate purely in digital spaces to intelligent agents that interact with and learn from the physical world. Physical AI is revolutionizing how we think about artificial intelligence, moving from abstract computational models to embodied systems that can navigate, manipulate, and learn in real-world environments.","sidebar":"tutorialSidebar"},"module-1/concepts":{"id":"module-1/concepts","title":"Core Concepts: Nodes, Topics, Services","description":"ROS 2 communication is built on three fundamental concepts that form the backbone of robotic systems: nodes, topics, and services. Understanding these concepts is essential for developing distributed robotic applications that can scale from simple single-robot systems to complex multi-robot environments. These concepts provide the foundation for building modular, maintainable, and scalable robotic systems that can handle the complexity of humanoid robots with their multiple sensors, actuators, and control systems.","sidebar":"tutorialSidebar"},"module-1/index":{"id":"module-1/index","title":"Module 1: The Robotic Nervous System (ROS 2)","description":"This module introduces the Robot Operating System 2 (ROS 2), the communication backbone that enables coordination between different components of a robotic system. ROS 2 serves as the nervous system of modern robotics, facilitating seamless communication between sensors, actuators, controllers, and higher-level decision-making algorithms. Understanding ROS 2 is crucial for developing complex robotic systems, particularly humanoid robots that require real-time coordination of multiple subsystems.","sidebar":"tutorialSidebar"},"module-1/rclpy-examples":{"id":"module-1/rclpy-examples","title":"rclpy Implementation Examples","description":"This section provides practical implementation examples using rclpy, the Python client library for ROS 2. These examples demonstrate the fundamental patterns of publisher-subscriber communication that form the backbone of robotic systems. Understanding these patterns is crucial for developing real-world robotic applications where multiple components must coordinate seamlessly.","sidebar":"tutorialSidebar"},"module-1/urdf-structures":{"id":"module-1/urdf-structures","title":"URDF Structures for Humanoid Joints/Links","description":"URDF (Unified Robot Description Format) is a fundamental component in robotics that enables the precise description of robot models in ROS. For humanoid robots, URDF serves as the blueprint that defines the physical structure, kinematic relationships, and visual representation of the robot. This format is essential for both simulation and real-world robot control, as it provides the necessary information for motion planning, collision detection, and visualization.","sidebar":"tutorialSidebar"},"module-2/index":{"id":"module-2/index","title":"Module 2: The Digital Twin","description":"This module explores the concept of digital twins in robotics - virtual representations of physical robots that enable simulation, testing, and validation. Digital twins are essential in modern robotics development, allowing engineers to test algorithms, validate designs, and train AI systems in safe, controlled virtual environments before deploying to physical hardware. This approach significantly reduces development time, costs, and risks associated with real-world testing.","sidebar":"tutorialSidebar"},"module-2/sensors":{"id":"module-2/sensors","title":"Sensors: LiDAR and Depth Cameras","description":"Sensors form the perceptual foundation of robotic systems, enabling robots to understand and interact with their environment. In digital twin environments, accurate sensor simulation is crucial for developing and testing perception algorithms that will eventually run on physical robots. This section covers two of the most important sensors for robotics: LiDAR for 360-degree spatial awareness and depth cameras for detailed 3D scene understanding.","sidebar":"tutorialSidebar"},"module-2/tools":{"id":"module-2/tools","title":"Tools: Gazebo & Unity","description":"In robotics simulation, two primary tools serve different but complementary purposes: Gazebo for physics simulation and Unity for high-quality rendering. Together, they form a comprehensive simulation environment that accurately models both the physical behavior of robots and their visual representation in realistic environments. The combination of accurate physics and photorealistic rendering is essential for creating simulation environments that can effectively bridge the gap between virtual and real-world robotics applications.","sidebar":"tutorialSidebar"},"module-3/index":{"id":"module-3/index","title":"Module 3: The AI-Robot Brain","description":"This module delves into the integration of AI systems with robotic platforms, focusing on NVIDIA Isaac Sim and navigation systems. The AI-robot brain represents the cognitive layer of robotic systems, where perception data is processed to generate intelligent behaviors and decision-making. This integration is crucial for creating autonomous robots that can operate effectively in complex, dynamic environments.","sidebar":"tutorialSidebar"},"module-3/isaac-sim":{"id":"module-3/isaac-sim","title":"NVIDIA Isaac Sim & Isaac ROS","description":"NVIDIA Isaac Sim and Isaac ROS represent cutting-edge tools for robotics development that leverage NVIDIA\'s GPU computing and AI capabilities. These tools are particularly valuable for developing advanced robotic systems that require high-performance perception, planning, and control algorithms. Isaac Sim provides photorealistic simulation environments with accurate physics, while Isaac ROS brings GPU-accelerated perception and AI capabilities directly into the ROS 2 ecosystem.","sidebar":"tutorialSidebar"},"module-3/nav2-movement":{"id":"module-3/nav2-movement","title":"Nav2 Stack for Bipedal Movement","description":"The Navigation2 (Nav2) stack represents the state-of-the-art in robotic navigation systems, providing comprehensive path planning, execution, and obstacle avoidance capabilities. For humanoid robots, adapting Nav2 for bipedal movement presents unique challenges that require specialized approaches to path planning, trajectory generation, and motion control. Unlike wheeled robots that can move smoothly in any direction, bipedal robots must navigate with the constraints of legged locomotion, including balance maintenance and step planning.","sidebar":"tutorialSidebar"},"module-4/index":{"id":"module-4/index","title":"Module 4: Vision-Language-Action (VLA)","description":"This module explores the integration of vision, language, and action systems in embodied AI. Vision-Language-Action (VLA) models represent the cutting edge of embodied intelligence, where robots can perceive their environment through vision, understand and generate language, and execute complex actions based on multimodal inputs. This integration enables robots to interact naturally with humans and perform complex tasks in unstructured environments.","sidebar":"tutorialSidebar"},"module-4/integration":{"id":"module-4/integration","title":"Integration: Whisper -> LLM -> ROS 2 Actions","description":"The integration of voice processing, language understanding, and robotic action execution forms the foundation of natural human-robot interaction. This pipeline enables robots to receive natural language commands, interpret them, and execute appropriate physical actions. The system creates a seamless flow from human speech to robot behavior, making robotic systems more accessible and intuitive to use.","sidebar":"tutorialSidebar"}}}')}}]);